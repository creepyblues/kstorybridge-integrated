#!/usr/bin/env python3
"""
Naver Webtoon Scraper using Selenium
Scrapes title information from Naver Webtoon website using browser automation
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import re
import time
from typing import List, Dict, Optional

class NaverWebtoonSeleniumScraper:
    def __init__(self, headless: bool = True):
        self.base_url = "https://comic.naver.com"
        self.driver = None
        self.setup_driver(headless)
    
    def setup_driver(self, headless: bool):
        """Setup Chrome WebDriver"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        except Exception as e:
            print(f"Error setting up Chrome driver: {e}")
            print("Please make sure ChromeDriver is installed: brew install chromedriver")
            raise
    
    def get_title_links(self, limit: Optional[int] = None) -> List[str]:
        """Get title links from the main webtoon page"""
        urls_to_try = [
            "https://comic.naver.com/webtoon/weekday",
            "https://comic.naver.com/webtoon"
        ]
        
        for url in urls_to_try:
            try:
                print(f"Trying URL: {url}")
                self.driver.get(url)
                
                # Wait for page to load
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # Additional wait for dynamic content
                time.sleep(3)
                
                # Look for "ìš”ì¼ë³„ ì „ì²´ ì›¹íˆ°" section first
                title_links = []
                weekday_section_found = False
                
                try:
                    # Find the "ìš”ì¼ë³„ ì „ì²´ ì›¹íˆ°" heading or similar
                    page_text = self.driver.find_element(By.TAG_NAME, "body").text
                    if "ìš”ì¼ë³„ ì „ì²´ ì›¹íˆ°" in page_text or "ìš”ì¼ë³„" in page_text:
                        print("Found weekday section indicator")
                        weekday_section_found = True
                except:
                    print("Weekday section text not found, proceeding with general search")
                
                # Get all links that contain titleId
                all_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='titleId=']")
                print(f"Found {len(all_links)} total links with titleId")
                
                # If we found weekday section, try to filter links that appear after it
                if weekday_section_found:
                    # Look for elements containing "ìš”ì¼ë³„" text
                    try:
                        weekday_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), 'ìš”ì¼ë³„')]")
                        if weekday_elements:
                            weekday_element = weekday_elements[0]
                            print(f"Found weekday section element: {weekday_element.text[:50]}...")
                            
                            # Get all titleId links that come after this element in the DOM
                            following_links = self.driver.find_elements(By.XPATH, 
                                f"//*[contains(text(), 'ìš”ì¼ë³„')]/following::*//a[contains(@href, 'titleId=')]")
                            print(f"Found {len(following_links)} links after weekday section")
                            
                            if following_links:
                                all_links = following_links  # Get all available titles, no limit
                    except Exception as e:
                        print(f"Error finding weekday section: {e}")
                
                # Process the links
                for element in all_links:
                    try:
                        href = element.get_attribute('href')
                        if href and 'titleId=' in href and '/webtoon/list' in href:
                            # Keep the entire URL including &tab= parameters
                            if href not in title_links:
                                title_links.append(href)
                                print(f"Found title link: {href}")
                    except Exception as e:
                        continue
                
                if title_links:
                    print(f"Found {len(title_links)} title links from {url}")
                    if limit:
                        title_links = title_links[:limit]
                    return title_links
                    
            except Exception as e:
                print(f"Error fetching from {url}: {e}")
                continue
        
        # Fallback to sample links for testing
        print("No links found, using sample links for testing...")
        sample_links = [
            "https://comic.naver.com/webtoon/list?titleId=183559",  # ì‹ ì˜ íƒ‘ (Tower of God)
            "https://comic.naver.com/webtoon/list?titleId=626907",  # ì™¸ëª¨ì§€ìƒì£¼ì˜ (Lookism)
            "https://comic.naver.com/webtoon/list?titleId=570503",  # ì—¬ì‹ ê°•ë¦¼ (True Beauty)
            "https://comic.naver.com/webtoon/list?titleId=748105",  # í™”ì‚°ê·€í™˜ (Return of the Mount Hua Sect)
            "https://comic.naver.com/webtoon/list?titleId=819217",  # ê°€ë¹„ì§€íƒ€ìž„ (Garbage Time)
            "https://comic.naver.com/webtoon/list?titleId=22897",   # ë§ˆìŒì˜ì†Œë¦¬ (Sound of Heart)
            "https://comic.naver.com/webtoon/list?titleId=335885",  # ê°“ ì˜¤ë¸Œ í•˜ì´ìŠ¤ì¿¨ (The God of High School)
            "https://comic.naver.com/webtoon/list?titleId=597447",  # ë…ë¦½ì¼ê¸° (Independence Log)
            "https://comic.naver.com/webtoon/list?titleId=679519",  # ìœ ë¯¸ì˜ ì„¸í¬ë“¤ (Yumi's Cells)  
            "https://comic.naver.com/webtoon/list?titleId=710751",  # ë‚˜ í˜¼ìžë§Œ ë ˆë²¨ì—… (Solo Leveling)
            "https://comic.naver.com/webtoon/list?titleId=654774",  # ì°¸êµìœ¡ (True Education)
            "https://comic.naver.com/webtoon/list?titleId=758037",  # ìž¬ë²Œì§‘ ë§‰ë‚´ì•„ë“¤ (The Youngest Son of a Conglomerate)
            "https://comic.naver.com/webtoon/list?titleId=783054",  # ê¹€ë¶€ìž¥ (Manager Kim)
            "https://comic.naver.com/webtoon/list?titleId=779809",  # ë‚˜ë…¸ë§ˆì‹  (Nano Machine)
            "https://comic.naver.com/webtoon/list?titleId=822042",  # í…œë¹¨ (Item Mall)
            "https://comic.naver.com/webtoon/list?titleId=818093",  # ë‚´ê°€ í‚¤ìš´ Sê¸‰ë“¤ (The S-Classes That I Raised)
            "https://comic.naver.com/webtoon/list?titleId=796268",  # ê°“ê²œ (God Game)
            "https://comic.naver.com/webtoon/list?titleId=747269",  # ì‹¸ì›€ë…í•™ (Viral Hit)
            "https://comic.naver.com/webtoon/list?titleId=789766",  # ê²¬ìŠµ ìš©ì‚¬ ì•Œë°” ì‹ ì²­ì„œ (Part-time Hero)
            "https://comic.naver.com/webtoon/list?titleId=759833",  # ë‚˜ëŠ” ê³„ê¸‰ìž¥ì´ë‹¤ (I Am the Rank)
            "https://comic.naver.com/webtoon/list?titleId=671674",  # í•˜ì´ë¸Œ (Hive)
            "https://comic.naver.com/webtoon/list?titleId=725110",  # í”„ë¦¬ìŠ¤íŠ¸ (Priest)
            "https://comic.naver.com/webtoon/list?titleId=799893",  # ì²´ì¸ì†Œìš°ë§¨ (Chainsaw Man)
            "https://comic.naver.com/webtoon/list?titleId=832351",  # ë¬´í•œì „ìƒ (Infinite Reincarnation)
            "https://comic.naver.com/webtoon/list?titleId=839004",  # ì•„ê°€ì”¨ë¥¼ ë¶€íƒí•´ (Please Take Care of the Lady)
            "https://comic.naver.com/webtoon/list?titleId=815094",  # ìŠ¤ìœ„íŠ¸í™ˆ (Sweet Home)
            "https://comic.naver.com/webtoon/list?titleId=789766",  # ê²¬ìŠµ ìš©ì‚¬ ì•Œë°” ì‹ ì²­ì„œ
            "https://comic.naver.com/webtoon/list?titleId=794456",  # í‚¬ëŸ¬ë¶„ì‹ (Killer Snack Bar)
            "https://comic.naver.com/webtoon/list?titleId=822856",  # í—¬ì°½ (Hell Chang)
            "https://comic.naver.com/webtoon/list?titleId=836447"   # ìœˆë“œë¸Œë ˆì´ì»¤ (Wind Breaker)
        ]
        
        if limit:
            sample_links = sample_links[:limit]
        
        return sample_links
    
    def clean_title_url(self, url: str) -> str:
        """Keep the URL as is, including tab parameter"""
        # Just ensure it's a full URL
        if url.startswith('/'):
            url = urljoin(self.base_url, url)
        return url
    
    def scrape_title_info(self, title_url: str) -> Dict:
        """Scrape detailed information from a title page"""
        try:
            print(f"Scraping: {title_url}")
            self.driver.get(title_url)
            
            # Wait for page to load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Additional wait to let JavaScript execute
            time.sleep(3)
            
            # Optional: Take a screenshot for debugging (disabled for speed)
            # screenshot_path = f"debug_screenshot_{title_url.split('titleId=')[1]}.png" 
            # self.driver.save_screenshot(screenshot_path)
            # print(f"  Screenshot saved: {screenshot_path}")
            
            title_data = {
                'url': title_url,
                'title_image': None,
                'title_name_kr': None,
                'art_author': None,
                'story_author': None,
                'original_author_kr': None,
                'likes': None,
                'tagline': None,
                'tags': [],
                'age_rating': None
            }
            
            # Extract title image - look for large images that could be covers
            all_images = self.driver.find_elements(By.TAG_NAME, "img")
            print(f"  Found {len(all_images)} images total")
            
            for img in all_images:
                src = img.get_attribute('src')
                alt = img.get_attribute('alt') or ""
                
                # Look for cover/thumbnail images
                if (src and ('thumb' in src.lower() or 'cover' in src.lower() or 
                            'title' in alt.lower() or 'ì¸ë„¤ì¼' in alt)):
                    title_data['title_image'] = src
                    print(f"  Found cover image: {src}")
                    break
            
            # Fallback to specific selectors
            if not title_data['title_image']:
                img_selectors = [
                    ".thumb img",
                    ".cover img", 
                    ".titleImg img",
                    "img[alt*='ì¸ë„¤ì¼']",
                    ".comic_info .thumb img"
                ]
                
                for selector in img_selectors:
                    try:
                        img_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        src = img_element.get_attribute('src')
                        if src:
                            title_data['title_image'] = src
                            print(f"  Found image with selector '{selector}': {src}")
                            break
                    except NoSuchElementException:
                        continue
            
            # Extract title name
            title_selectors = [
                "h1",
                "h2", 
                ".comicinfo .detail h2",
                ".comic_info h2",
                ".title_area h2",
                "h2.title",
                ".titleArea h2",
                ".ContentTitle h2",
                ".WebtoonTitleHeader h1"
            ]
            
            # First, let's see what h1 and h2 elements exist
            all_h1_h2 = self.driver.find_elements(By.CSS_SELECTOR, "h1, h2")
            print(f"  Found {len(all_h1_h2)} h1/h2 elements:")
            for i, elem in enumerate(all_h1_h2[:5]):  # Show first 5
                text = elem.text.strip()
                if text:
                    print(f"    {i+1}: {text[:50]}...")
            
            # Look for the actual webtoon title from the h1/h2 elements we found
            for elem in all_h1_h2:
                text = elem.text.strip()
                # Skip navigation elements and look for actual webtoon titles
                if (text and len(text) > 2 and len(text) < 100 and 
                    'NAVER' not in text and 'ì›¹íˆ°' not in text and 
                    'ì›¹ì†Œì„¤' not in text and 'ì‹œë¦¬ì¦ˆ' not in text and
                    'ê´€ë ¨ ìƒí’ˆ' not in text and 'ìž‘ê°€ì˜ ë‹¤ë¥¸ ìž‘í’ˆ' not in text and
                    'ë…ìžë“¤ì´ ë§Žì´ ë³¸' not in text):
                    # Clean up the title by removing status indicators
                    clean_title = re.sub(r'\n(íœ´ìž¬|ì™„ê²°|ì‹ ìž‘|UP).*$', '', text).strip()
                    title_data['title_name_kr'] = clean_title
                    print(f"  Found title: {clean_title}")
                    break
            
            # Fallback to selectors if no title found
            if not title_data['title_name_kr']:
                for selector in title_selectors:
                    try:
                        title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        title_text = title_element.text.strip()
                        if title_text and len(title_text) > 2:
                            title_data['title_name_kr'] = title_text
                            print(f"  Found title with selector '{selector}': {title_text}")
                            break
                    except NoSuchElementException:
                        continue
            
            # Get all text content for pattern matching
            all_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            # Debug: Look for specific author patterns in the text
            print(f"  Looking for author information...")
            
            # Extract authors using regex patterns with more specific matching
            # Handle combined ê¸€/ê·¸ë¦¼ (same person for art and story) - name appears BEFORE ê¸€/ê·¸ë¦¼
            combined_author_match = re.search(r'([ê°€-íž£a-zA-Z]+)\s*âˆ™?\s*ê¸€/ê·¸ë¦¼', all_text)
            if combined_author_match:
                author_name = combined_author_match.group(1).strip()
                if author_name and len(author_name) < 20:  # Korean names are typically shorter
                    title_data['art_author'] = author_name
                    title_data['story_author'] = author_name
                    print(f"  Found combined art/story author: {author_name}")
            else:
                # Try individual patterns if no combined author found
                author_patterns = {
                    'art_author': [
                        # Format: "name âˆ™ ê·¸ë¦¼" (name before role)
                        r'([ê°€-íž£a-zA-Z]+)\s*âˆ™\s*ê·¸ë¦¼',
                        # Format: "ê·¸ë¦¼: name" or "ê·¸ë¦¼ name" (role before name)
                        r'ê·¸ë¦¼\s*[:ï¼š]?\s*([ê°€-íž£a-zA-Z]+)(?=\s|$|\n|/|ê·¸ë¦¼|ê¸€|ì›ìž‘|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€)',
                        r'ê·¸ë¦¼\s+([ê°€-íž£a-zA-Z]+)(?=\s|$|\n|/|ê·¸ë¦¼|ê¸€|ì›ìž‘|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€)'
                    ],
                    'story_author': [
                        # Format: "name âˆ™ ê¸€" (name before role)
                        r'([ê°€-íž£a-zA-Z]+)\s*âˆ™\s*ê¸€',
                        # Format: "ê¸€: name" or "ê¸€ name" (role before name)
                        r'ê¸€\s*[:ï¼š]?\s*([ê°€-íž£a-zA-Z]+)(?=\s|$|\n|/|ê·¸ë¦¼|ê¸€|ì›ìž‘|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€)',
                        r'ê¸€\s+([ê°€-íž£a-zA-Z]+)(?=\s|$|\n|/|ê·¸ë¦¼|ê¸€|ì›ìž‘|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€)'
                    ]
                }
                
                for field, patterns in author_patterns.items():
                    found_author = False
                    for pattern in patterns:
                        if found_author:  # Skip remaining patterns if we already found a valid author
                            break
                        matches = re.findall(pattern, all_text)
                        for match in matches:
                            author_name = match.strip()
                            # Filter out unwanted text - Korean names are typically 2-4 characters
                            if (author_name and len(author_name) > 1 and len(author_name) < 20 and 
                                not re.match(r'^\d+í™”', author_name) and
                                'ì™„ê²°' not in author_name and 'íœ´ìž¬' not in author_name and
                                'ì´ë²¤íŠ¸' not in author_name and 'ì•ˆë‚´' not in author_name and
                                'í™•ì¸' not in author_name and 'í•„ìš”' not in author_name and
                                'ë‹¹ì²¨' not in author_name and 'ì›”ìš”ì›¹íˆ°' not in author_name and
                                'ê¸ˆìš”ì›¹íˆ°' not in author_name):
                                title_data[field] = author_name
                                print(f"  Found {field}: {author_name}")
                                found_author = True
                                break
            
            # Extract original author
            original_author_patterns = [
                # Format: "name âˆ™ ì›ìž‘" (name before role)
                r'([ê°€-íž£a-zA-Z]+)\s*âˆ™\s*ì›ìž‘',
                # Format: "ì›ìž‘: name" or "ì›ìž‘ name" (role before name)
                r'ì›ìž‘\s*[:ï¼š]?\s*([ê°€-íž£a-zA-Z\s]+?)(?=\s|$|\n|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€|ê¸€|ê·¸ë¦¼)',
                r'ì›ìž‘\s+([ê°€-íž£a-zA-Z\s]+?)(?=\s|$|\n|ì „ì²´ì—°ë ¹|ì„¸\s*ì´ìš©ê°€|ê¸€|ê·¸ë¦¼)'
            ]
            
            for pattern in original_author_patterns:
                matches = re.findall(pattern, all_text)
                for match in matches:
                    author_name = match.strip()
                    if (author_name and len(author_name) > 1 and len(author_name) < 50 and 
                        not re.match(r'^\d+í™”', author_name)):
                        title_data['original_author_kr'] = author_name
                        print(f"  Found original_author_kr: {author_name}")
                        break
                if title_data['original_author_kr']:
                    break
            
            # Extract likes - handle numbers with commas
            like_patterns = [r'\+ê´€ì‹¬\s*([\d,]+)', r'ê´€ì‹¬\s*([\d,]+)', r'ì¢‹ì•„ìš”\s*([\d,]+)']
            for pattern in like_patterns:
                like_match = re.search(pattern, all_text)
                if like_match:
                    # Remove commas and convert to int
                    likes_str = like_match.group(1).replace(',', '')
                    title_data['likes'] = int(likes_str)
                    print(f"  Found likes: {title_data['likes']}")
                    break
            
            # Extract age rating
            age_patterns = [
                r'ì „ì²´ì—°ë ¹ê°€',  # All ages
                r'(\d+)ì„¸\s*ì´ìš©ê°€', 
                r'(\d+)ì„¸\s*ê´€ëžŒê°€'
            ]
            for pattern in age_patterns:
                age_match = re.search(pattern, all_text)
                if age_match:
                    if pattern == r'ì „ì²´ì—°ë ¹ê°€':
                        title_data['age_rating'] = "ì „ì²´ì—°ë ¹ê°€"
                    else:
                        title_data['age_rating'] = f"{age_match.group(1)}ì„¸ ì´ìš©ê°€"
                    print(f"  Found age rating: {title_data['age_rating']}")
                    break
            
            # Extract tags
            tag_matches = re.findall(r'#[ê°€-íž£a-zA-Z0-9_]+', all_text)
            if tag_matches:
                title_data['tags'] = list(set(tag_matches))
                print(f"  Found tags: {title_data['tags']}")
            
            # Extract tagline/summary - text between age rating and hashtags
            # First try to find text that appears between age rating and tags (most accurate)
            tagline_patterns = [
                # Pattern: age_rating ... tagline ... #tags
                r'(?:ì „ì²´ì—°ë ¹ê°€|(?:\d+)ì„¸\s*ì´ìš©ê°€)\s+([^#]*?)(?=\s*#|$)',
                # Long multi-paragraph storylines (for webtoons like the third example) 
                r'ë¬´ê³µì— ë¯¸ì¹œ[^#]{50,800}(?=#|$)',  # Specific pattern for martial arts stories
                r'([ê°€-íž£][^#]{100,800}(?:\.|\?)(?:\s*[ê°€-íž£][^#]{50,400}(?:\.|\?))*)',  # Multi-sentence Korean descriptions
                r'\"([^\"]{20,300})\"',  # Text in quotes - expanded range
                r'\"([^\"]*(?:\([^)]*\))*[^\"]{20,300})\"',  # Text in quotes with parentheses  
                r'ë‚˜ëŠ”\s+([^.]{30,200}\.)',  # Text starting with "ë‚˜ëŠ”" (I am/I...)
                r'ê·¸\w*\s+([^.]{30,200}\.)',  # Text starting with "ê·¸" (He/She/It...)
                r'([^.]{30,200}\.[^.]{30,200}\.)',  # Multi-sentence description
            ]
            
            for pattern in tagline_patterns:
                tagline_matches = re.findall(pattern, all_text, re.DOTALL)
                for match in tagline_matches:
                    text = match.strip()
                    # Filter out unwanted matches
                    if (text and len(text) > 30 and len(text) < 300 and 
                        not text.startswith('#') and 
                        'ì´ìš©ê°€' not in text and 'ê´€ì‹¬' not in text and
                        'ê¸€/ê·¸ë¦¼' not in text and 'ì›ìž‘' not in text and
                        'https://' not in text and 'www.' not in text and
                        'ê³µì§€ì‚¬í•­' not in text and 'ì›¹íˆ°' not in text and
                        'NAVER' not in text):
                        # Clean up the text
                        clean_text = re.sub(r'\s+', ' ', text).strip()
                        if clean_text:
                            title_data['tagline'] = clean_text
                            print(f"  Found tagline: {clean_text[:50]}...")
                            break
                if title_data['tagline']:
                    break
            
            # Fallback: try CSS selectors
            if not title_data['tagline']:
                summary_selectors = [
                    ".detail .summary",
                    ".comic_info .summary", 
                    ".description",
                    ".intro"
                ]
                
                for selector in summary_selectors:
                    try:
                        summary_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        summary_text = summary_element.text.strip()
                        if summary_text and len(summary_text) > 30:
                            title_data['tagline'] = summary_text
                            print(f"  Found tagline (CSS): {summary_text[:50]}...")
                            break
                    except NoSuchElementException:
                        continue
            
            return title_data
            
        except Exception as e:
            print(f"Error scraping title {title_url}: {e}")
            return {'url': title_url, 'error': str(e)}
    
    def scrape_titles(self, limit: int = 10) -> List[Dict]:
        """Scrape multiple titles with rate limiting"""
        title_links = self.get_title_links(limit)
        print(f"Found {len(title_links)} title links")
        
        results = []
        for i, link in enumerate(title_links):
            print(f"Scraping title {i+1}/{len(title_links)}: {link}")
            title_data = self.scrape_title_info(link)
            results.append(title_data)
            
            # Rate limiting - optimized for large scale scraping
            if i < len(title_links) - 1:
                time.sleep(0.5)  # Reduced delay for faster scraping
                
            # Progress indicator and intermediate saving for large scraping runs
            if (i + 1) % 10 == 0:
                print(f"Progress: {i+1}/{len(title_links)} titles completed ({((i+1)/len(title_links)*100):.1f}%)")
                
            # Save intermediate results every 50 titles to prevent data loss
            if (i + 1) % 50 == 0:
                temp_output = f"naver_webtoon_partial_{i+1}.json"
                with open(temp_output, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"  ðŸ’¾ Intermediate results saved to {temp_output}")
        
        return results
    
    def close(self):
        """Close the webdriver"""
        if self.driver:
            self.driver.quit()

def main():
    """Main function to run the scraper"""
    scraper = None
    try:
        scraper = NaverWebtoonSeleniumScraper(headless=True)  # Run in headless mode for speed
        
        print("Starting Naver Webtoon Selenium scraper...")
        print("Scraping ALL available titles with enhanced parser...")
        
        results = scraper.scrape_titles(limit=None)  # Remove limit to scrape all titles
        
        # Save results to JSON file
        output_file = "naver_webtoon_all_titles.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸŽ‰ Scraping complete! Results saved to {output_file}")
        print(f"Successfully scraped {len(results)} titles")
        
        # Generate summary statistics
        successful_titles = [r for r in results if r.get('title_name_kr') and r.get('title_name_kr') != 'ë„¤ì´ë²„']
        titles_with_images = [r for r in successful_titles if r.get('title_image')]
        titles_with_authors = [r for r in successful_titles if r.get('art_author') or r.get('story_author')]
        titles_with_taglines = [r for r in successful_titles if r.get('tagline')]
        titles_with_likes = [r for r in successful_titles if r.get('likes')]
        
        print(f"\nðŸ“Š Summary Statistics:")
        print(f"  Total titles processed: {len(results)}")
        print(f"  Valid webtoon titles: {len(successful_titles)}")
        print(f"  Titles with cover images: {len(titles_with_images)} ({len(titles_with_images)/len(successful_titles)*100:.1f}%)")
        print(f"  Titles with author info: {len(titles_with_authors)} ({len(titles_with_authors)/len(successful_titles)*100:.1f}%)")
        print(f"  Titles with taglines: {len(titles_with_taglines)} ({len(titles_with_taglines)/len(successful_titles)*100:.1f}%)")
        print(f"  Titles with like counts: {len(titles_with_likes)} ({len(titles_with_likes)/len(successful_titles)*100:.1f}%)")
        
        # Display summary
        for i, title in enumerate(results[:3]):
            print(f"\nTitle {i+1}:")
            print(f"  Name: {title.get('title_name_kr', 'N/A')}")
            print(f"  Authors: Art: {title.get('art_author', 'N/A')}, Story: {title.get('story_author', 'N/A')}")
            print(f"  Age Rating: {title.get('age_rating', 'N/A')}")
            print(f"  Tags: {title.get('tags', [])}")
            print(f"  Error: {title.get('error', 'None')}")
            
    except Exception as e:
        print(f"Error running scraper: {e}")
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()